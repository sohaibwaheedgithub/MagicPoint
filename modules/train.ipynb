{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 23:14:01.964816: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741976041.978965    1591 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741976041.984574    1591 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-14 23:14:02.001108: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "I0000 00:00:1741976044.913425    1591 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5582 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import io\n",
    "import sys\n",
    "import keras\n",
    "import random\n",
    "from glob import glob\n",
    "from constants import *\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import buildMagicPoint\n",
    "from google.protobuf import text_format\n",
    "from tensorboard.plugins import projector\n",
    "from tensorflow.python.platform import gfile\n",
    "from data_preparation import DataPreparation\n",
    "from metrics import CornerDetectionAveragePrecision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining paths\n",
    "project_dir = \"/mnt/c/Users/SohaibWaheed/Desktop/Personal_Projects/Final_SuperPoint\"\n",
    "dataset_dir = project_dir + \"/dataset/tfrecords\"\n",
    "model_dir = project_dir + \"/nd_he_normal_standardized_saved_models\" #model_dir = project_dir + \"/saved_models\"\n",
    "log_dir = project_dir + \"/nd_he_normal_standardized_logs\" #log_dir = project_dir + \"/logs\"\n",
    "validation_tfrecord = dataset_dir + \"/valid/valid_record_no_1.tfrecord\"\n",
    "train_log_dir = log_dir + '/train'\n",
    "valid_log_dir = log_dir + '/valid'\n",
    "projector_config_path = train_log_dir + \"/projector_config.pbtxt\"\n",
    "embeddings_path = train_log_dir + \"/embeddings.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "valid_summary_writer = tf.summary.create_file_writer(valid_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sohaibwaheed/anaconda3/envs/superpoint/lib/python3.12/site-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'shared_encoder', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/home/sohaibwaheed/anaconda3/envs/superpoint/lib/python3.12/site-packages/keras/src/layers/layer.py:391: UserWarning: `build()` was called on layer 'interest_point_decoder', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# To calculate the round number to start from\n",
    "model_files = os.listdir(model_dir)\n",
    "projector_config = projector.ProjectorConfig()\n",
    "\n",
    "if len(model_files) == 0:\n",
    "    roundStart = 1\n",
    "    # Initialzing model if there is no previous model found    \n",
    "    magicPoint = buildMagicPoint(input_shape=MP_INPUT_SHAPE)\n",
    "    \n",
    "    magicPoint.compile(\n",
    "        optimizer=keras.optimizers.Adam(),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy()\n",
    "    )\n",
    "else:\n",
    "    roundStart = max([int(file[:-6].split(\"_\")[-1]) for file in model_files]) + 1\n",
    "    last_model_file = f\"magicPoint_{roundStart-1}.keras\"\n",
    "    # Loading saved model\n",
    "    magicPoint = keras.models.load_model(model_dir + \"/\" + last_model_file)\n",
    "    # Delete previous files\n",
    "    model_files.pop(model_files.index(last_model_file))\n",
    "    '''for file in model_files:\n",
    "        os.remove(model_dir + \"/\" + file)'''\n",
    "        \n",
    "    # To Load the existing .pbtxt file\n",
    "    with gfile.GFile(projector_config_path, 'r') as f:\n",
    "        text_format.Merge(f.read(), projector_config)\n",
    "        \n",
    "\n",
    "roundEnd = 445\n",
    "# Initialize the dataset\n",
    "dataPreparation = DataPreparation(MP_BATCH_SIZE)\n",
    "\n",
    "# Validation Dataset\n",
    "validDataset = dataPreparation.loadDataset([validation_tfrecord])\n",
    "validDatasetIterator = validDataset.__iter__()\n",
    "    \n",
    "\n",
    "metric = CornerDetectionAveragePrecision()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "\n",
    "'''datasetIterator = validDataset.__iter__()\n",
    "\n",
    "test = True\n",
    "while test:\n",
    "    tf.config.run_functions_eagerly(False)\n",
    "    images, points, bins = datasetIterator.__next__()\n",
    "    modelOutput = magicPoint(images)[\"finalOutput\"]\n",
    "    tf.config.run_functions_eagerly(True)\n",
    "    metric.update_state(points, modelOutput)\n",
    "    for m, value in metric.result().items():\n",
    "        print(f\"{m}: {value.numpy()}\")\n",
    "    metric.reset_state()\n",
    "    answer = input(\"Want to evaluate another batch ? (y/n): \")\n",
    "    if answer.lower() == \"y\":\n",
    "        test = True\n",
    "    else:\n",
    "        test = False''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradients(images, bins):\n",
    "    '''\n",
    "    To get model's current state output along with gradients\n",
    "    \n",
    "    Args:\n",
    "        images [tf.Tensor]: batch of input images\n",
    "        bins [tf.Tensor]: batch of respective bins present in each image in the batch\n",
    "    \n",
    "    Returns:\n",
    "        output: MagicPoint model output\n",
    "        gradients [tf.Tensor]: gradients of the current state of the moodel\n",
    "    '''\n",
    "    \n",
    "    # Sample Weights Calculation\n",
    "    n_points = tf.reduce_sum(tf.where(bins != 64, 1, 0), axis=[1, 2])\n",
    "    n_points = tf.where(n_points == 0, 300, n_points)\n",
    "\n",
    "    height, width = MP_INPUT_SHAPE[0] // 8, MP_INPUT_SHAPE[1] // 8\n",
    "    n_npoints = tf.subtract(height * width, n_points)\n",
    "\n",
    "    pointsWeights = tf.divide(n_npoints, n_points)\n",
    "\n",
    "    n_npointsWeights = (1 / (pointsWeights + 1))[..., tf.newaxis, tf.newaxis]\n",
    "    n_npointsWeights = tf.broadcast_to(n_npointsWeights, shape=[MP_BATCH_SIZE, height, width])\n",
    "\n",
    "    pointsWeights = (pointsWeights / (pointsWeights + 1))[..., tf.newaxis, tf.newaxis]\n",
    "    pointsWeights = tf.broadcast_to(pointsWeights, shape=[MP_BATCH_SIZE, height, width])\n",
    "\n",
    "    sample_weights = tf.where(bins != 64, pointsWeights, n_npointsWeights)\n",
    "\n",
    "    # Because want to visualize gradients along the training, so performing inferencing with the GradientTape context\n",
    "    with tf.GradientTape() as tape:\n",
    "        output = magicPoint(images, training=False)\n",
    "        loss = magicPoint.compute_loss(\n",
    "            y=bins,\n",
    "            y_pred=output[\"interestPointDecoderOutput\"],\n",
    "            sample_weight=sample_weights,\n",
    "        )\n",
    "    \n",
    "    # Trainable Parameters of First, Middle and Last Convolutional Layers of SE And First And Last Convolutional Layers of IPD\n",
    "    trainable_vars = magicPoint.layers[2].SEConvBlock_1.conv2d_1.trainable_variables + \\\n",
    "        magicPoint.layers[2].SEConvBlock_2.conv2d_2.trainable_variables + \\\n",
    "        magicPoint.layers[2].SEConvBlock_4.conv2d_2.trainable_variables + \\\n",
    "        magicPoint.layers[3].conv2d.trainable_variables + \\\n",
    "        magicPoint.layers[3].bottleNeckLayer.trainable_variables\n",
    "        \n",
    "    gradients = tape.gradient(loss, trainable_vars)\n",
    "    \n",
    "    return output, gradients\n",
    "\n",
    "\n",
    "def plot_to_image(figure):\n",
    "    '''\n",
    "    To convert matplotlib figure to tensorflow image so that it can be visualized in tensorboard\n",
    "    \n",
    "    Args:\n",
    "        figure [plt.figure]: matplotlib figure containing all images in a batch\n",
    "    \n",
    "    Returns:\n",
    "        image [tf.Tensor]: figure converted to tensorflow image\n",
    "    '''\n",
    "    buffer = io.BytesIO()\n",
    "    plt.savefig(buffer, format=\"png\")\n",
    "    plt.close(figure)\n",
    "    buffer.seek(0)\n",
    "    image = tf.image.decode_png(buffer.getvalue(), channels=4)\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 409/445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 06:31:43.785619: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:370] TFRecordDataset `buffer_size` is unspecified, default to 262144\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1741743105.924259    2574 service.cc:148] XLA service 0x7fdcbc06fb40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1741743105.924562    2574 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 3070 Laptop GPU, Compute Capability 8.6\n",
      "2025-03-12 06:31:46.042119: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1741743106.565934    2574 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  1/450\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29:47\u001b[0m 12s/step - loss: 0.0103"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1741743116.169291    2574 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 338ms/step - loss: 0.0072 - val_loss: 0.0069\n",
      "Round 410/445\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 84ms/step - loss: 0.0072 - val_loss: 0.0068\n",
      "Round 411/445\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 65ms/step - loss: 0.0068 - val_loss: 0.0069\n",
      "Round 412/445\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 66ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Round 413/445\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 75ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Round 414/445\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 67ms/step - loss: 0.0070 - val_loss: 0.0068\n",
      "Round 415/445\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 83ms/step - loss: 0.0070 - val_loss: 0.0068\n",
      "Round 416/445\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 85ms/step - loss: 0.0071 - val_loss: 0.0068\n",
      "Round 417/445\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 67ms/step - loss: 0.0072 - val_loss: 0.0068\n",
      "Round 418/445\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 67ms/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Round 419/445\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 75ms/step - loss: 0.0071 - val_loss: 0.0068\n",
      "Round 420/445\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 67ms/step - loss: 0.0067 - val_loss: 0.0068\n",
      "Round 421/445\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 67ms/step - loss: 0.0071 - val_loss: 0.0068\n",
      "Round 422/445\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 63ms/step - loss: 0.0072 - val_loss: 0.0069\n",
      "Round 423/445\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 66ms/step - loss: 0.0070 - val_loss: 0.0068\n",
      "Round 424/445\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 67ms/step - loss: 0.0070 - val_loss: 0.0068\n",
      "Round 425/445\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 76ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Round 426/445\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 74ms/step - loss: 0.0070 - val_loss: 0.0068\n",
      "Round 427/445\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 76ms/step - loss: 0.0070 - val_loss: 0.0068\n",
      "Round 428/445\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 66ms/step - loss: 0.0068 - val_loss: 0.0069\n",
      "Round 429/445\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 66ms/step - loss: 0.0069 - val_loss: 0.0069\n",
      "Round 430/445\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 63ms/step - loss: 0.0072 - val_loss: 0.0070\n",
      "Round 431/445\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 66ms/step - loss: 0.0069 - val_loss: 0.0067\n",
      "Round 432/445\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 67ms/step - loss: 0.0067 - val_loss: 0.0069\n",
      "Round 433/445\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 74ms/step - loss: 0.0070 - val_loss: 0.0068\n",
      "Round 434/445\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 74ms/step - loss: 0.0072 - val_loss: 0.0068\n",
      "Round 435/445\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 66ms/step - loss: 0.0068 - val_loss: 0.0069\n",
      "Round 436/445\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 75ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Round 437/445\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 66ms/step - loss: 0.0068 - val_loss: 0.0068\n",
      "Round 438/445\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 66ms/step - loss: 0.0069 - val_loss: 0.0067\n",
      "Round 439/445\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 66ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Round 440/445\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 67ms/step - loss: 0.0069 - val_loss: 0.0068\n",
      "Round 441/445\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 67ms/step - loss: 0.0069 - val_loss: 0.0067\n",
      "Round 442/445\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 74ms/step - loss: 0.0069 - val_loss: 0.0066\n",
      "Round 443/445\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 66ms/step - loss: 0.0069 - val_loss: 0.0067\n",
      "Round 444/445\n",
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 66ms/step - loss: 0.0069 - val_loss: 0.0067\n",
      "Round 445/445\n",
      "\u001b[1m186/450\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - loss: 0.0068"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 07:12:56.068142: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "2025-03-12 07:12:56.068403: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[IteratorGetNext/_6]]\n",
      "/home/sohaibwaheed/anaconda3/envs/superpoint/lib/python3.12/contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m450/450\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 33ms/step - loss: 0.0068 - val_loss: 0.0068\n"
     ]
    }
   ],
   "source": [
    "embeddings_dict = {}\n",
    "for round in range(roundStart, roundEnd+1):\n",
    "    # Defining File paths for this iteration\n",
    "    metadata_fp = os.path.join(train_log_dir, f\"metadata_{round}.tsv\")\n",
    "    sprite_fp = os.path.join(train_log_dir, f\"sprite_{round}.png\")\n",
    "    \n",
    "    trainDataset = dataPreparation.loadDataset([dataset_dir + f\"/train/train_record_no_{round}.tfrecord\"])\n",
    "    \n",
    "    print(f\"Round {round}/{roundEnd}\")\n",
    "    \n",
    "    images, points, bins = validDatasetIterator.__next__()\n",
    "    # To get the shape [batch_size, [bins in each image]]\n",
    "    flattened_bins = tf.reshape(bins, [MP_BATCH_SIZE, -1])\n",
    "    imgs_bins = tf.ragged.boolean_mask(flattened_bins, mask=flattened_bins!=64)\n",
    "    # Saving labels (bins) for each image\n",
    "    with open(metadata_fp, \"w\") as f:\n",
    "        for img_bins in imgs_bins:\n",
    "            img_bins = list(map(str, img_bins.numpy()))\n",
    "            if len(img_bins) == 0:\n",
    "                f.write(\"None\")\n",
    "            else:\n",
    "                f.write(\",\".join(img_bins))\n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "    # Generate random images\n",
    "    # Create a sprite image\n",
    "    sprite_height, sprite_width = 8, 4 #Square grid dimensions\n",
    "    sprite_image = tf.Variable(tf.ones((sprite_height * MP_INPUT_SHAPE[0], sprite_width * MP_INPUT_SHAPE[1])))\n",
    "    # assigning each image to it's respective position in the grid \n",
    "    for idx, img in enumerate(images):\n",
    "        row = idx // sprite_width\n",
    "        col = idx % sprite_width\n",
    "        sprite_image[\n",
    "            row * MP_INPUT_SHAPE[0] : (row + 1) * MP_INPUT_SHAPE[0],\n",
    "            col * MP_INPUT_SHAPE[1] : (col + 1) * MP_INPUT_SHAPE[1]\n",
    "        ].assign(img[:, :, 0]/255)\n",
    "\n",
    "    # Save sprite image\n",
    "    plt.imsave(sprite_fp, sprite_image, cmap=\"gray\")\n",
    "\n",
    "    history = magicPoint.fit(\n",
    "        trainDataset, \n",
    "        epochs=1, \n",
    "        steps_per_epoch=450, \n",
    "        validation_data=validDataset,\n",
    "        validation_steps=450,\n",
    "        callbacks=[\n",
    "            keras.callbacks.ModelCheckpoint(\n",
    "                filepath=model_dir + f\"/magicPoint_{round}.keras\",\n",
    "                monitor=\"val_loss\",\n",
    "                save_best_only=False,\n",
    "                save_weights_only=False\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    del trainDataset    \n",
    "    \n",
    "    modelOutput, gradients = compute_gradients(images, bins)\n",
    "    # extracting penultimate layer embeddings for visualization\n",
    "    embeddings_name = f\"embeddings_round_{round}\"\n",
    "    embeddings = tf.Variable(tf.reshape(modelOutput[\"sharedEncoder\"], shape=[MP_BATCH_SIZE, -1]), name=embeddings_name)\n",
    "    embeddings_dict[embeddings_name] = embeddings\n",
    "    \n",
    "    # Add embedding configuration\n",
    "    embedding = projector_config.embeddings.add()\n",
    "    embedding.tensor_name = f\"embeddings_round_{round}\" + \"/.ATTRIBUTES/VARIABLE_VALUE\"\n",
    "    embedding.metadata_path = metadata_fp\n",
    "    \n",
    "    # Add sprite image configuration (if applicable)\n",
    "    embedding.sprite.image_path = sprite_fp\n",
    "    embedding.sprite.single_image_dim.extend(MP_INPUT_SHAPE[-2::-1])\n",
    "    \n",
    "    \n",
    "    # To update CornerDetectionAveragePrecision state\n",
    "    tf.config.run_functions_eagerly(True)\n",
    "    metric.update_state(points, modelOutput[\"finalOutput\"])\n",
    "    tf.config.run_functions_eagerly(False)\n",
    "    metric_results = metric.result()\n",
    "    \n",
    "    \n",
    "    # Retrieving activations of first convolutional layer of SE and generating it's figure\n",
    "    random_index = random.randint(0, MP_BATCH_SIZE-1)\n",
    "    firstLayerActivations = magicPoint.layers[2].SEConvBlock_1.conv2d_1(magicPoint.layers[1](images[random_index:random_index+1]))\n",
    "    activationsFigure = plt.figure()\n",
    "    for idx, filter in enumerate(tf.transpose(firstLayerActivations[0], perm=[2, 0, 1]), 1):\n",
    "        activationsFigure.add_subplot(8, 8, idx)\n",
    "        plt.imshow(filter, cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "    activationsImage = plot_to_image(activationsFigure)\n",
    "        \n",
    "    # Retrieving Kernel of first convolutional layer of SE and generating it's figure\n",
    "    firstLayerKernel = magicPoint.layers[2].SEConvBlock_1.conv2d_1.kernel\n",
    "    kernelFigure = plt.figure()\n",
    "    for idx, filter in enumerate(tf.transpose(firstLayerKernel, perm=[3, 0, 1, 2]), 1):\n",
    "        kernelFigure.add_subplot(8, 8, idx)\n",
    "        plt.imshow(filter, cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "    kernelImage = plot_to_image(kernelFigure)\n",
    "    \n",
    "    \n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', history.history[\"loss\"][0], step=round)\n",
    "        \n",
    "        # To write the kernel, bias and gradients of first, middle and layers convolutional layers of Shared Encoder\n",
    "        tf.summary.histogram(\"SE First Convolutional Layer Kernel\", magicPoint.layers[2].SEConvBlock_1.conv2d_1.kernel, step=round)\n",
    "        tf.summary.histogram(\"SE First Convolutional Layer Bias\", magicPoint.layers[2].SEConvBlock_1.conv2d_1.bias, step=round)\n",
    "        tf.summary.histogram(\"SE First Convolutional Layer Kernel Gradients\", gradients[0], step=round)\n",
    "        tf.summary.histogram(\"SE First Convolutional Layer Bias Gradients\", gradients[1], step=round)\n",
    "        \n",
    "        tf.summary.histogram(\"SE Middle Convolutional Layer Kernel\", magicPoint.layers[2].SEConvBlock_2.conv2d_2.kernel, step=round)\n",
    "        tf.summary.histogram(\"SE Middle Convolutional Layer Bias\", magicPoint.layers[2].SEConvBlock_2.conv2d_2.bias, step=round)\n",
    "        tf.summary.histogram(\"SE Middle Convolutional Layer Kernel Gradients\", gradients[2], step=round)\n",
    "        tf.summary.histogram(\"SE Middle Convolutional Layer Bias Gradients\", gradients[3], step=round)\n",
    "        \n",
    "        tf.summary.histogram(\"SE Last Convolutional Layer Kernel\", magicPoint.layers[2].SEConvBlock_4.conv2d_2.kernel, step=round)\n",
    "        tf.summary.histogram(\"SE Last Convolutional Layer Bias\", magicPoint.layers[2].SEConvBlock_4.conv2d_2.bias, step=round)\n",
    "        tf.summary.histogram(\"SE Last Convolutional Layer Kernel Gradients\", gradients[4], step=round)\n",
    "        tf.summary.histogram(\"SE Last Convolutional Layer Bias Gradients\", gradients[5], step=round)\n",
    "        \n",
    "        \n",
    "        # To write the kernel and bias of first and last convolutional layer of Interest Point Decoder\n",
    "        tf.summary.histogram(\"IPD First Convolutional Layer Kernel\", magicPoint.layers[3].conv2d.kernel, step=round)\n",
    "        tf.summary.histogram(\"IPD First Convolutional Layer Bias\", magicPoint.layers[3].conv2d.bias, step=round)\n",
    "        tf.summary.histogram(\"IPD First Convolutional Layer Kernel Gradients\", gradients[6], step=round)\n",
    "        tf.summary.histogram(\"IPD First Convolutional Layer Bias Gradients\", gradients[7], step=round)\n",
    "        \n",
    "        tf.summary.histogram(\"IPD Last Convolutional Layer Kernel\", magicPoint.layers[3].bottleNeckLayer.kernel, step=round)\n",
    "        tf.summary.histogram(\"IPD Last Convolutional Layer Bias\", magicPoint.layers[3].bottleNeckLayer.bias, step=round)\n",
    "        tf.summary.histogram(\"IPD Last Convolutional Layer Kernel Gradients\", gradients[8], step=round)\n",
    "        tf.summary.histogram(\"IPD Last Convolutional Layer Bias Gradients\", gradients[9], step=round)\n",
    "        \n",
    "        # To write the images of activations of first layer of SE\n",
    "        tf.summary.image(\"SE First Layer Activations\", activationsImage, step=round)\n",
    "        # To write the images of Kernel of first layer of SE\n",
    "        tf.summary.image(\"SE First Layer Kernel\", kernelImage, step=round)        \n",
    "        \n",
    "    \n",
    "        \n",
    "    with valid_summary_writer.as_default():\n",
    "        tf.summary.scalar(\"val_loss\", history.history[\"val_loss\"][0], step=round)\n",
    "        tf.summary.scalar(\"Mean Average Precision\", metric_results[\"mAP\"].numpy(), step=round)\n",
    "        tf.summary.scalar(\"Mean Localization Error\", metric_results[\"mLE\"].numpy(), step=round)\n",
    "        \n",
    "    metric.reset_state()\n",
    "    \n",
    "    # Save Embeddings Checkpoint and projector configuration file\n",
    "    for i in range(1, roundStart):\n",
    "        embeddings_placeholder = tf.Variable(tf.zeros_like(embeddings), name=f\"embeddings_round_{i}\")\n",
    "        embeddings_dict[f\"embeddings_round_{i}\"] = embeddings_placeholder\n",
    "    checkpoint = tf.train.Checkpoint(**embeddings_dict)\n",
    "    # Loading latest checkpoint to restore old embeddings\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(train_log_dir)\n",
    "    if not latest_checkpoint is None:\n",
    "        checkpoint.restore(latest_checkpoint).expect_partial()\n",
    "        old_ckpt_files = glob(os.path.join(train_log_dir, f\"embeddings.ckpt-{round-1}*\"))\n",
    "        os.remove(old_ckpt_files[0])\n",
    "        os.remove(old_ckpt_files[1])\n",
    "    checkpoint.save(os.path.join(train_log_dir, f\"embeddings.ckpt\"))\n",
    "\n",
    "    # Save configuration file\n",
    "    projector.visualize_embeddings(train_log_dir, projector_config)\n",
    "\n",
    "    K.clear_session()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 23:19:26.047188: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741976366.061489    6978 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741976366.066327    6978 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-14 23:19:26.081733: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.18.0 at http://localhost:6008/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system(f\"tensorboard --logdir {log_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "superpoint",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
